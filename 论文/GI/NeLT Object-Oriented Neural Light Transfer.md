# 摘要
提出了一种神经渲染的新方式，逐物体插入场景以逐步更新光照

# 数学基础
## GI问题
GI要解决的问题可以写为以下函数：
$$
L_{in}(\mathcal{\omega},\mathcal{o})=G(\mathcal{\omega},\mathcal{o};\mathcal{S},\mathcal{L})
$$
表示在$\mathcal{o}$位置向$\mathcal{\omega}$方向的辐射度由一个传播函数$G$决定，这个函数除了接受方向和位置参数之外，还需要场景$\mathcal{S}$和光源参数$\mathcal{L}$
## 面向物体的传播函数
本文提出了一种新的对传播函数进行拆解的方法：
![](论文/GI/pics/6.png)
其中m表示在场景中添加的新物体，这一公式实质上可以写为如下方式：
$$
T_m(\mathcal{\omega},\mathcal{o};\mathcal{S},\mathcal{L})\left\{
  \begin{array}{ll}
    G(\mathcal{\omega},\mathcal{o};\mathcal{S}+m,\mathcal{L}) & \text{如果 }m\text{可见} \\
    G(\mathcal{\omega},\mathcal{o};\mathcal{S}+m,\mathcal{L})-G(\mathcal{\omega},\mathcal{o};\mathcal{S},\mathcal{L}) & \text{如果} m \text{不可见}
  \end{array}
\right.
$$
即这一传播函数描述的是插入前到插入后的光照变化情况。文章将这一**传播函数使用神经网络实现**。

## 对GI的分解
将GI分解为直接光和间接光，传播函数也做此分解：
![](论文/GI/pics/7.png)
与间接光照不同，直接光照采用了比值而非差?，这两部分互不影响，并且分开存储。
![](论文/GI/pics/8.png)
有了上述分解，就可以分别递推求解两个全局光照信息：
$$
\begin{array}{ll}
G^1(\mathcal{S}+m)=M_mT^1(\mathcal{S})+(1-M_m)T^1(\mathcal{S})G^1(\mathcal{S})\\
G^*(\mathcal{S}+m)=M_mT^*(\mathcal{S})+(1-M_m)(T^*(\mathcal{S})+G^*(\mathcal{S}))
\end{array}
$$
整个渲染过程如下图所示：
![](论文/GI/pics/10.png)
# 网络设计
整个网络的示意如下图：
![](论文/GI/pics/11.png)
## 参数表示
传播函数$T$在实现中表示如下：
![](论文/GI/pics/9.png)
其中$g(\mathcal{x})$为GBuffer，而$z_l$和$z_s$分别表示光源信息和场景信息的隐式表示
### 场景表示
对于场景可分为前景（即要添加的物体）和背景（即已添加的物体）
**文章对背景做了一个假设，即从前景的中心点出发，360度看一圈，其中看不到的区域对间接光照没有贡献（不然的话背景的信息太多了处理不过来）**。然后从前景的中心出发做6次光栅化得到的GBuffer，输入背景编码器进行编码得到背景表示$\mathcal{z_b}$。
而对于前景而言，即便这个神经网络是逐物体的，但对物体表面的材质信息仍是缺失的，因此将放置一些随机采样点在物体表面，采样到的材质信息输入编码器得到前景表示$\mathcal{z_f}$
### 光源表示
文章处理了两种光源：面光源和环境光，对于这两种光源，对其进行重要性采样得到多个样本，每个样本记录以下信息：位置、法线（对于环境光，位置是一个足够远的虚拟位置）、发射的辐射度（辐射度根据平均辐射度进行规范化）和采样概率。得到这些样本之后将其输入光源编码器进行编码得到初步的光源表示。
获得初步的光源表示之后，要考虑光源的可见性信息。而直接光和间接光的可见性需要的考虑不同，因此使用两个不同的网络来对光源描述符进行赋权。
对于直接光而言，无需考虑背景信息中的材质信息（只需考虑几何和遮挡），因此将背景的GBuffer中的位置和法线单独分出来编码得到遮挡表示，将其输入直接光聚合网络中；而对于间接光而言，则可直接使用背景表示+光源表示输入间接光聚合网络中。
### 网络结构
为了减少网络的参数量和推理负担，本文采用了一种超网络结构。以间接光为例，具体来说如下：
1. 将场景和光源信息输入神经纹理生成器，生成一张神经纹理
2. 将场景和光源信息输入UV超网络，生成UV生成器所用的参数
3. 将场景和光源信息输入间接光传播函数超网络，生成用于间接光的传播函数生成器所用的参数
4. 将GBuffer输入到已有参数的UV生成器中，得到神经纹理上的UV
5. 采样神经纹理，得到的描述和GBuffer和视线方向一起输入传播函数生成器得到传播函数
整个过程如下所示：
![](论文/GI/pics/13.png)
对于直接光也是类似的，只是直接光输出了三个传播函数，分别用于Specular，Diffuse和阴影，如下图：
![](论文/GI/pics/14.png)
# 实验
实验的BaseLine选择了Compositional Neural Scene Representation (CNSR)和Active Exploration (AE)
其余略
# 总结
## 局限
+ 推理时间较长
+ 无法解决Deformable的物体
+ 对高频信息的还原有限
+ 传播函数的划分不平衡，越往后的传播函数隐含的场景、物体信息越多，并且整个场景需以一个特定的顺序添加物体