# 摘要
展示了一种基于哈希的空间相关信息编码方式，可以用较少的内存空间将较为复杂的空间相关网络信息压缩起来，以提高MLP的预测准确度。

# 方法
对于一个神经网络$n(y;\Phi)$，本文要做的工作是将空间位置输入$x$映射成$y$，即实现：$y=enc(x;\theta)$。这个映射通过多个哈希网格来进行实现，具体来说，是有：
+ $L$个不同分辨率的网格
+ 每个网格有$T$个格
+ 每个格中保存着一个$F$维向量
## 哈希网格设计
本文的网格中的每个格代表着一个区域，第$l$层的网格分辨率为$N_l$，当规定好了$L$个不同分辨率的网格，并规定最小层和最大层网格的分辨率$N_{min}$和$N_{max}$之后，其余网格的分辨率由以下式子计算：
![](26.png)
对于$d$维空间中任意一个位置输入，其会与每一层的网格中的$2^d$个相邻网格产生关联，计算过程如下：
+ 先用位置$\times$分辨率，对每个分量进行上取整和下取整，得到两个整数向量，这两个向量有$2^d$种组合，得到$2^d$个关联格子。
$$
\begin{array}\\
\lfloor x_l\rfloor=\lfloor x\cdot N_l\rfloor\\
\lceil x_l\rceil=\lceil x\cdot N_l\rceil
\end{array}
$$
+ 对于每一层的网格来说，最多以哈希表的形式保存$T$个特征格点，因此对于稀疏网格而言，$(N_l+1)^d<T$，因此这些格子可以一一映射；对于稠密网格而言，需要的格子数量比实际存储的多，因此需要使用哈希函数来对其进行哈希，并且不显式处理冲突。这个哈希函数如下：
![](27.png)
相当于每个分量乘上一个大质数，结果进行异或，最后取模得到了存储位置。
+ 得到$2^d$个特征之后之后，进行线性插值，得到编码的结果
## 哈希冲突处理
首先，由于哈希函数选取的原因，可以认为空间中发生冲突的的点是均匀随机分布的。并且统计发现在不同的层，同一个点并不总冲突。这就是说，训练时梯度是均匀的。也就是说，即便发生冲突，梯度叠加到了同一个格点上，也总是梯度大的点影响更大。并且的话，由于采用了多重分辨率网格，而且同点不同网格不会总碰撞，因此保证了学习的有效性。
# 结果
为了展示这一方法的通用性，文章选取了以下一些应用场景来进行实验：
![](28.png)
## 大分辨率图片近似
即让神经网络去学习一个$2D\rightarrow RGB$的映射。
对比当时的SOTA方法ACORN，使用了分辨率网格的方法可以减少MLP的复杂度。
## SDF学习
对比了NGLOD的SOTA方法以及Nerf的学习方法，本文方法的性能近似SOTA
## NRC
对比了原NRC论文，本文方法对高频阴影等信息的学习更好。
## Nerf
对比了nerf、mip-Nerf和NSVF，达到相同的PSNR水平，前者训练1~5min，后者只需要训练15s。
![](29.png)
# 结论
## 一些细节和未来方向
+ 多个网格输出的哈希输出的神经特征本文将其拼接起来而非将其统一为一个特征。文章的实验得出拼接起来效果比较好。
+ 哈希函数改进为更适合进行微分的函数